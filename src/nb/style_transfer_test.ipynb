{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"data\" # This may need to be changed on different machines\n",
    "\n",
    "# Make sure we're in the correct directory and make sure the data directory exists\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.chdir(\"../..\") # Move up two directories because we're in src/nb and the data directory/path should be in/start at the root directory \n",
    "    assert os.path.exists(DATA_DIR), f\"ERROR: DATA_DIR={DATA_DIR} not found\"  # If we still can't see the data directory something is wrong\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "# get Dataset class\n",
    "from src.lib.decoder import Decoder\n",
    "from src.lib.paraphrase_model import Paraphraser\n",
    "from src.lib.style_classifier import StyleEncoder\n",
    "from src.lib.util import to_device\n",
    "from transformers import GPT2LMHeadModel, AdamW, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# class InputBuilder:\n",
    "\n",
    "#     def __init__(self, state_dict):\n",
    "#         if state_dict is not None:\n",
    "#             self.positional_embeds = state_dict[\"positional_embeds\"] # \n",
    "#             self.token_embeds = state_dict[\"token_embeds\"] #\n",
    "#             return\n",
    "        \n",
    "#         paraphraser = Paraphraser()\n",
    "#         # Just look up these weights\n",
    "#         self.positional_embeds = paraphraser.model.transformer.wpe.weight\n",
    "#         self.token_embeds = paraphraser.model.transformer.wte.weight\n",
    "\n",
    "#         # put everything on the cpu\n",
    "#         self.positional_embeds = self.positional_embeds.to(\"cpu\").detach()\n",
    "#         self.token_embeds = self.token_embeds.to(\"cpu\").detach()\n",
    "\n",
    "#         self.stye_encoder = StyleEncoder()\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "#     def build_input(self, semantic_sentence, stlye_sentence):\n",
    "\n",
    "#         style_encoding = self.stye_encoder.get_style_vector([stlye_sentence])\n",
    "#         style_encoding = style_encoding.to(\"cpu\").detach()\n",
    "\n",
    "\n",
    "        \n",
    "#         style = self.df[\"label\"][idx]\n",
    "#         random_style_encoding_idx = np.random.choice(np.where(self.df[\"label\"] == style)[0])\n",
    "#         style_encoding = self.style_encodings[random_style_encoding_idx]\n",
    "\n",
    "#         para_ids = self.para_ids[idx]\n",
    "#         para_embed = self.token_embeds[para_ids]\n",
    "#         para_attn_mask = self.para_attn[idx]\n",
    "\n",
    "#         target_ids = self.target_ids[idx]\n",
    "#         target_embeds = self.token_embeds[target_ids]\n",
    "#         target_attn_mask = self.target_attn[idx].detach().clone()\n",
    "\n",
    "#         # select a random non-padding text index\n",
    "#         selected_idx = np.random.choice(np.where(target_attn_mask == 1)[0])\n",
    "#         # set the text_token_attn_mask to 0 starting at the selected index\n",
    "#         target_attn_mask[selected_idx:] = 0\n",
    "\n",
    "#         para_length = torch.sum(para_attn_mask).item()\n",
    "#         para_pos_ids = np.arange(0, len(para_ids))\n",
    "#         para_pos = self.positional_embeds[para_pos_ids]\n",
    "\n",
    "#         bos_pos_id = para_length + 1\n",
    "#         bos_pos = self.positional_embeds[bos_pos_id]\n",
    "\n",
    "#         target_pos_ids = np.arange(para_length + 2, para_length + 2 + len(target_ids))\n",
    "#         target_pos = self.positional_embeds[target_pos_ids]\n",
    "\n",
    "#         # attn_mask = torch.tensor([[1], para_attn_mask, [1], target_attn_mask])\n",
    "#         attn_mask = torch.ones(2 + len(para_attn_mask) + len(target_attn_mask))\n",
    "#         attn_mask[1:len(para_attn_mask)+1] = para_attn_mask\n",
    "#         attn_mask[len(para_attn_mask)+2:] = target_attn_mask\n",
    "\n",
    "#         label = target_ids[selected_idx]\n",
    "#         label_idx = len(para_ids) + 2 + selected_idx\n",
    "\n",
    "#         style_encoding = style_encoding.detach()\n",
    "#         para_embed = para_embed.detach()\n",
    "#         para_pos = para_pos.detach()\n",
    "#         target_embeds = target_embeds.detach()\n",
    "#         target_pos = target_pos.detach()\n",
    "#         attn_mask = attn_mask.detach()\n",
    "#         label = label.detach()\n",
    "#         bos_pos = bos_pos.detach()\n",
    "\n",
    "#         return (\n",
    "#             style_encoding, # Style encoding form BERT style classification of the target sentence\n",
    "#             (para_embed, para_pos), # Token embeddings from the paraphrased sentence with positional embeddings\n",
    "#             bos_pos, # Positional embedding of the BOS token\n",
    "#             (target_embeds, target_pos), # Token embeddings from the target sentence with positional embeddings\n",
    "#             attn_mask # Attention mask for the entire sequence\n",
    "#         ), (\n",
    "#             label, # Token id of the token to be predicted - index in vocab\n",
    "#             label_idx # Index in the sequence of the token to be predicted - index in the sequence\n",
    "#         )\n",
    "    \n",
    "\n",
    "#     def save_state_dict(self, path):\n",
    "#         state = {\n",
    "#             \"df\": self.df,\n",
    "#             \"style_encodings\": self.style_encodings,\n",
    "#             \"positional_embeds\": self.positional_embeds,\n",
    "#             \"token_embeds\": self.token_embeds,\n",
    "#             \"para_ids\": self.para_ids,\n",
    "#             \"para_attn\": self.para_attn,\n",
    "#             \"target_ids\": self.target_ids,\n",
    "#             \"target_attn\": self.target_attn\n",
    "#         }\n",
    "#         torch.save(state, path)\n",
    "    \n",
    "\n",
    "#     @classmethod\n",
    "#     def from_state_dict(cls, path):\n",
    "#         state = torch.load(path)\n",
    "#         return cls(state_dict=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_decoder(state_dict_path):\n",
    "    state_dict = torch.load(state_dict_path)\n",
    "    for key in state_dict:\n",
    "        state_dict[key] = state_dict[key].cpu()\n",
    "    decoder = Decoder()\n",
    "    decoder.load_state_dict(state_dict)\n",
    "    return decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/gpt2_large were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.bias', 'transformer.extra_embedding_project.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = \"training_results/decoder_0_0.0979/model.pth\"\n",
    "decoder = load_decoder(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_encoder = StyleEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_encoding = style_encoder.get_style_vector([style_sentence]).squeeze(0)\n",
    "# style_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional_embeds = decoder.gpt2.transformer.wpe.weight\n",
    "# token_embeds = decoder.gpt2.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(semantic_sentence, style_sentence, style_encoder, decoder):\n",
    "    positional_embeds = decoder.gpt2.transformer.wpe.weight\n",
    "    token_embeds = decoder.gpt2.transformer.wte.weight\n",
    "\n",
    "    style_encoding = style_encoder.get_style_vector([style_sentence]).squeeze(0) # (768)\n",
    "\n",
    "    max_length = 50\n",
    "    tokenized = decoder.tokenizer([semantic_sentence], return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    para_ids = tokenized[\"input_ids\"] # (1, para_length)\n",
    "    para_attn = tokenized[\"attention_mask\"].squeeze(0) # (para_length)\n",
    "\n",
    "    para_embeds = token_embeds[para_ids].squeeze(0).detach() # (para_length, 1280)\n",
    "    para_pos = positional_embeds[np.arange(0, len(para_embeds))].detach() # (para_length, 1280)\n",
    "\n",
    "    target_embeds = token_embeds[[decoder.tokenizer.bos_token_id, decoder.tokenizer.pad_token_id]].detach() # (2, 1280)\n",
    "    target_pos = positional_embeds[[len(para_ids), len(para_ids)+1]].detach() # (2, 1280)\n",
    "\n",
    "    target_attn = torch.tensor([1, 0])\n",
    "\n",
    "    attn_mask = torch.ones(2 + len(para_attn) + len(target_attn)) # (2 + para_length + target_length)\n",
    "    attn_mask[1:len(para_attn)+1] = para_attn\n",
    "    attn_mask[len(para_attn)+2:] = target_attn # just one for the BOS token\n",
    "    attn_mask = attn_mask\n",
    "\n",
    "    bos_pos = positional_embeds[len(para_ids) + 1].detach()\n",
    "\n",
    "    style_encoding = style_encoding.unsqueeze(0)\n",
    "    para_embeds = para_embeds.unsqueeze(0)\n",
    "    para_pos = para_pos.unsqueeze(0)\n",
    "    bos_pos = bos_pos.unsqueeze(0)\n",
    "    target_embeds = target_embeds.unsqueeze(0)\n",
    "    target_pos = target_pos.unsqueeze(0)\n",
    "    attn_mask = attn_mask.unsqueeze(0)\n",
    "\n",
    "    x = (\n",
    "        style_encoding,\n",
    "        (para_embeds, para_pos),\n",
    "        bos_pos,\n",
    "        (target_embeds, target_pos),\n",
    "        attn_mask.unsqueeze(0)\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def generate(x, decoder, truncate=False, max_length=50, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x = to_device(x, device)\n",
    "    decoder = decoder.to(device)\n",
    "    \n",
    "    # print sum of attn_mask\n",
    "    \n",
    "\n",
    "    generated_ids = []\n",
    "    generated_logits = []\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        while i < max_length or not truncate:\n",
    "            i += 1\n",
    "\n",
    "            style_encoding, para, bos_pos, target, attn_mask = x\n",
    "            target_embeds, target_pos = target\n",
    "\n",
    "            print(attn_mask.sum())\n",
    "\n",
    "            output = decoder(x)\n",
    "\n",
    "            logits = output.logits[0, -1, :]\n",
    "            token_id = logits.argmax()\n",
    "\n",
    "            generated_logits.append(logits)\n",
    "\n",
    "\n",
    "            # check if token_id is eos\n",
    "            if token_id == decoder.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # add generated id \n",
    "            generated_ids.append(token_id.item())\n",
    "            next_embedding = decoder.gpt2.transformer.wte.weight[token_id]\n",
    "            next_pos_embed = decoder.gpt2.transformer.wpe.weight[target_pos.shape[1] + 1]\n",
    "\n",
    "            # update the target embedding\n",
    "            target_embeds = torch.cat([target_embeds, next_embedding.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            # update the target position\n",
    "            target_pos = torch.cat([target_pos, next_pos_embed.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            # update the attention mask\n",
    "            attn_mask = torch.cat([attn_mask, torch.ones(1, 1, 1).to(device)], dim=2)\n",
    "\n",
    "            # repackage x\n",
    "            x = (\n",
    "                style_encoding,\n",
    "                para,\n",
    "                bos_pos,\n",
    "                (target_embeds, target_pos),\n",
    "                attn_mask\n",
    "            )\n",
    "        \n",
    "    return generated_ids, generated_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_sentence = \"Hello, how are you?\"\n",
    "style_sentence = \"The all-seeing sun Ne'er saw her match since first the world begun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., device='cuda:0')\n",
      "tensor(10., device='cuda:0')\n",
      "tensor(11., device='cuda:0')\n",
      "tensor(12., device='cuda:0')\n",
      "tensor(13., device='cuda:0')\n",
      "tensor(14., device='cuda:0')\n",
      "tensor(15., device='cuda:0')\n",
      "tensor(16., device='cuda:0')\n",
      "tensor(17., device='cuda:0')\n",
      "tensor(18., device='cuda:0')\n",
      "tensor(19., device='cuda:0')\n",
      "tensor(20., device='cuda:0')\n",
      "tensor(21., device='cuda:0')\n",
      "tensor(22., device='cuda:0')\n",
      "tensor(23., device='cuda:0')\n",
      "tensor(24., device='cuda:0')\n",
      "tensor(25., device='cuda:0')\n",
      "tensor(26., device='cuda:0')\n",
      "tensor(27., device='cuda:0')\n",
      "tensor(28., device='cuda:0')\n",
      "tensor(29., device='cuda:0')\n",
      "tensor(30., device='cuda:0')\n",
      "tensor(31., device='cuda:0')\n",
      "tensor(32., device='cuda:0')\n",
      "tensor(33., device='cuda:0')\n",
      "tensor(34., device='cuda:0')\n",
      "tensor(35., device='cuda:0')\n",
      "tensor(36., device='cuda:0')\n",
      "tensor(37., device='cuda:0')\n",
      "tensor(38., device='cuda:0')\n",
      "tensor(39., device='cuda:0')\n",
      "tensor(40., device='cuda:0')\n",
      "tensor(41., device='cuda:0')\n",
      "tensor(42., device='cuda:0')\n",
      "tensor(43., device='cuda:0')\n",
      "tensor(44., device='cuda:0')\n",
      "tensor(45., device='cuda:0')\n",
      "tensor(46., device='cuda:0')\n",
      "tensor(47., device='cuda:0')\n",
      "tensor(48., device='cuda:0')\n",
      "tensor(49., device='cuda:0')\n",
      "tensor(50., device='cuda:0')\n",
      "tensor(51., device='cuda:0')\n",
      "tensor(52., device='cuda:0')\n",
      "tensor(53., device='cuda:0')\n",
      "tensor(54., device='cuda:0')\n",
      "tensor(55., device='cuda:0')\n",
      "tensor(56., device='cuda:0')\n",
      "tensor(57., device='cuda:0')\n",
      "tensor(58., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_sequence = build_input(semantic_sentence, style_sentence, style_encoder, decoder)\n",
    "tokens, logits = generate(input_sequence, decoder, device=device, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(15365, device='cuda:0')]\n",
      "[tensor(15365, device='cuda:0')]\n",
      "[tensor(15365, device='cuda:0')]\n",
      "[tensor(15365, device='cuda:0')]\n",
      "[tensor(15365, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print([logits[0].argmax()])\n",
    "print([logits[1].argmax()])\n",
    "print([logits[2].argmax()])\n",
    "print([logits[3].argmax()])\n",
    "print([logits[4].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTSENTS'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode the tokens\n",
    "text = decoder.tokenizer.decode(tokens)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "tokenized = decoder.tokenizer([semantic_sentence], return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "para_ids = tokenized[\"input_ids\"]\n",
    "para_attn = tokenized[\"attention_mask\"].squeeze(0)\n",
    "print(para_ids.shape)\n",
    "print(para_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_embeds = token_embeds[para_ids].squeeze(0).detach()\n",
    "para_pos = positional_embeds[np.arange(0, len(para_embeds))].detach()\n",
    "# para_embeds += para_pos\n",
    "print(para_embeds.shape)\n",
    "print(para_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeds = token_embeds[decoder.tokenizer.bos_token_id].unsqueeze(0).detach()\n",
    "target_pos = positional_embeds[len(para_ids)].unsqueeze(0).detach()\n",
    "# target_embeds += target_pos\n",
    "print(target_pos.shape)\n",
    "print(target_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attn = torch.tensor([1])\n",
    "\n",
    "attn_mask = torch.ones(2 + len(para_attn) + len(target_attn))\n",
    "attn_mask[1:len(para_attn)+1] = para_attn\n",
    "attn_mask[len(para_attn)+2:] = target_attn # just one for the BOS token\n",
    "attn_mask = attn_mask\n",
    "print(attn_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_pos = positional_embeds[len(para_ids) + 1].detach()\n",
    "print(bos_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"style_encoding\", style_encoding.shape)\n",
    "print(\"para_embeds\", para_embeds.shape)\n",
    "print(\"para_pos\", para_pos.shape)\n",
    "print(\"bos_pos\", bos_pos.shape)\n",
    "print(\"target_embeds\", target_embeds.shape)\n",
    "print(\"target_pos\", target_pos.shape)\n",
    "print(\"attn_mask\", attn_mask.shape)\n",
    "\n",
    "# unsqueeze to add batch dimension\n",
    "style_encoding = style_encoding.unsqueeze(0)\n",
    "para_embeds = para_embeds.unsqueeze(0)\n",
    "para_pos = para_pos.unsqueeze(0)\n",
    "bos_pos = bos_pos.unsqueeze(0)\n",
    "target_embeds = target_embeds.unsqueeze(0)\n",
    "target_pos = target_pos.unsqueeze(0)\n",
    "attn_mask = attn_mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (\n",
    "    style_encoding,\n",
    "    (para_embeds, para_pos),\n",
    "    bos_pos,\n",
    "    (target_embeds, target_pos),\n",
    "    attn_mask.unsqueeze(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = to_device(x, \"cuda\")\n",
    "decoder = decoder.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = output.logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5854a06c42c722c9b376878a2b0ccc4f0377baad6105251f5d0bcbe5a7e06c30"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
