{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"data\" # This may need to be changed on different machines\n",
    "\n",
    "# Make sure we're in the correct directory and make sure the data directory exists\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.chdir(\"../..\") # Move up two directories because we're in src/nb and the data directory/path should be in/start at the root directory \n",
    "    assert os.path.exists(DATA_DIR), f\"ERROR: DATA_DIR={DATA_DIR} not found\"  # If we still can't see the data directory something is wrong\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "# get Dataset class\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2LMHeadModel, AdamW, GPT2Tokenizer\n",
    "\n",
    "from src.lib.decoder_dataset import DecoderDataset\n",
    "from src.lib.decoder import Decoder\n",
    "from src.lib.util import to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name in [\"dev\", \"test\", \"train\"]:\n",
    "    save_path = os.path.join(DATA_DIR, \"decoded_cds\", \"balanced\", f\"{dataset_name}_dataset.pth\")    \n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, \"decoded_cds\", \"balanced\", f\"{dataset_name}.csv\"), index_col=0)\n",
    "        dataset = DecoderDataset(df)\n",
    "        dataset.save_state_dict(save_path)\n",
    "    else:\n",
    "        dataset = DecoderDataset.from_state_dict(save_path)\n",
    "    \n",
    "    datasets[dataset_name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/gpt2_large were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.bias', 'transformer.extra_embedding_project.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "learning_rate = 5e-5\n",
    "optimizer_name = \"AdamW\"\n",
    "\n",
    "\n",
    "\n",
    "data_loaders = {}\n",
    "for dataset_name in datasets:\n",
    "    data_loaders[dataset_name] = DataLoader(datasets[dataset_name], batch_size=batch_size, num_workers=10)\n",
    "\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a51d6a352104c19b99b0d01857552ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    decoder.train()\n",
    "    pbar = tqdm(data_loaders[\"train\"])\n",
    "    for batch in pbar:\n",
    "        batch = to_device(batch, device)\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        label, label_idx = y\n",
    "\n",
    "        logits = decoder(x).logits[:, label_idx].diagonal().t()\n",
    "\n",
    "        # calculate loss and backprop\n",
    "        loss = loss_fn(logits, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_value = loss.item()\n",
    "        loss_history.append(loss_value)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch} Loss: {np.mean(loss_history[-30:]):.4f}\")\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        decoder.eval()\n",
    "        total_val_loss = 0\n",
    "        val_loss_count = 0\n",
    "        for batch in data_loaders[\"dev\"]:\n",
    "            batch = to_device(batch, device)\n",
    "\n",
    "            x, y = batch\n",
    "\n",
    "            label, label_idx = y\n",
    "\n",
    "            logits = decoder(x).logits[:, label_idx].diagonal().t()\n",
    "\n",
    "            loss = loss_fn(logits, label)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            total_val_loss += loss_value\n",
    "            val_loss_count += 1\n",
    "        \n",
    "        val_loss_history.append(total_val_loss / val_loss_count)\n",
    "    \n",
    "    # save model checkpoint\n",
    "    avg_loss = np.mean(loss_history[-len(data_loaders[\"train\"]):])\n",
    "    model_name = f\"decoder_{epoch}_{avg_loss:.4f}\"\n",
    "    save_path = os.path.join(\"decoder_checkpoints\", \"balanced\", model_name)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path) \n",
    "    torch.save(decoder.state_dict(), os.path.join(save_path, \"model.pth\"))\n",
    "    # save loss histories\n",
    "    histories = {\n",
    "        \"loss_history\": loss_history,\n",
    "        \"val_loss_history\": val_loss_history\n",
    "    }\n",
    "    with open(os.path.join(save_path, \"history.json\"), \"w\") as f:\n",
    "        json.dump(histories, f)\n",
    "    # save config\n",
    "    config = {\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"optimizer\": optimizer_name,\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
